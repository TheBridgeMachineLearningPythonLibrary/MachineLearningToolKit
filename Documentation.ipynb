{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1><b>ds11mltoolkit</b></h1></center>\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><b><i>Library developed by the Data Science class of 2022-2023</b></i></h3>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index of contents\n",
    "<a id=\"indice\"></a>\n",
    "\n",
    "\n",
    "* [Introduction](#seccion-1)\n",
    "* [Installation and basic usage](#seccion-2)\n",
    "* [Library Framework](#seccion-3)\n",
    "* [Functions desciptions](#seccion-4)\n",
    "  * [Data Analysis functions](#seccion-5)\n",
    "    * [read_url](#seccion-6)\n",
    "    * [read_csv_zip](#seccion-7)\n",
    "    * [chi_squared_test](#seccion-8)\n",
    "  * [Data Processing fuctions](#seccion-9)\n",
    "    * [list_categorical_columns](#seccion-10)\n",
    "    * [last_columns](#seccion-11)\n",
    "    * [uniq_value](#seccion-12)\n",
    "    * [load_imgs](#seccion-13)\n",
    "    * [class ImageDataGen(ImageDataGenerator) 3-in-1 functions](#seccion-14)\n",
    "    * [clean_text](#seccion-15)\n",
    "    * [processing_model_classification](#seccion-18)\n",
    "    * [replace_convert_numeric](#seccion-19)\n",
    "    * [log_transform_numeric](#seccion-21)\n",
    "    * [add_previous](#seccion-22)\n",
    "    * [_exponential_smooth](#seccion-24)\n",
    "    * [Nan_treatment](#seccion-25)\n",
    "    * [convert_to_numeric](#seccion-26)\n",
    "    * [auto_dtype_converter](#seccion-27)\n",
    "    * [winner_loser](#seccion-32)\n",
    "    * [lstm_model](#seccion-55)\n",
    "  * [Machine Learning functions](#seccion-28)\n",
    "    * [export_model](#seccion-16)\n",
    "    * [import_model](#seccion-17)\n",
    "    * [worst_params](#seccion-23)\n",
    "    * [load_model_zip](#seccion-20)\n",
    "    * [quickregression](#seccion-29)\n",
    "    * [polynomial_features_non_binary](#seccion-30)\n",
    "    * [worst_params](#seccion-31)\n",
    "    * [balance_binary_target](#seccion-33)\n",
    "    * [image_scrap](#seccion-34)\n",
    "    * [create_multiclass_prediction_df](#seccion-36)\n",
    "    * [show_scoring](#seccion-37)\n",
    "    * [predict_model_classification](#seccion-38)\n",
    "    * [UnsupervisedKMeans](#seccion-39)\n",
    "    * [UnsupervisedPCA](#seccion-40)\n",
    "  * [Plot functions](#seccion-41)\n",
    "    * [heatmap](#seccion-42)\n",
    "    * [sunburst](#seccion-43)\n",
    "    * [correl_map_max](#seccion-44)\n",
    "    * [plot_map](#seccion-53)\n",
    "    * [plot_ngram](#seccion-45)\n",
    "    * [wordcloudviz](#seccion-46)\n",
    "    * [plot_cumulative_variance_ratio](#seccion-47)\n",
    "    * [plot_roc_cruve](#seccion-48)\n",
    "    * [plot_multiclass_prediction_image](#seccion-49)\n",
    "* [Packaging and Deployment Description](#seccion-50)\n",
    "* [Publish on PyPI](#seccion-51)\n",
    "* [Last conclusions](#seccion-52)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a class=\"anchor\" id=\"seccion-1\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to the documentation for `ds11mltoolkit`!\n",
    "\n",
    "ds11mltoolkit is a Python library that provides a set of tools for data analysis, preprocessing, machine learning, and visualization. This library was created with data scientist developers in mind, who are looking for an easily accessible and customizable set of functions for their data analysis projects.\n",
    "\n",
    "ds11mltoolkit offers a wide range of functions and methods to work with data, including loading and manipulating data, exploratory data analysis, cleaning and preprocessing, feature selection, and building and evaluating machine learning models. Additionally, it also includes tools for visualizing data and the results of analysis and modeling.\n",
    "\n",
    "In the official documentation for ds11mltoolkit, we will cover the following points:\n",
    "\n",
    "> *Installation*: detailed instructions on how to install the library, including prerequisites and necessary dependencies.\n",
    ">\n",
    "> *Basic usage*: an introduction to the basic functions and methods of the library, including code examples.\n",
    ">\n",
    "> *Data analysis*: description of the data analysis functions, such as loading, manipulation, cleaning, and preprocessing.\n",
    ">\n",
    "> *Data preprocessing*: explanation of data preprocessing techniques and their implementations in the library.\n",
    ">\n",
    "> *Modeling*: detailed explanation of the modeling and machine learning techniques included in the library, as well as code examples.\n",
    ">\n",
    "> *Visualization*: description of visualization functions and their use in presenting data analysis and modeling results.\n",
    ">\n",
    "> *Contributions*: instructions for users interested in contributing to the project, including how to collaborate and report errors or suggestions for improvement.\n",
    ">\n",
    "> *References*: a list of useful references for working with the library, including links to related external resources.\n",
    "\n",
    "We hope that ds11mltoolkit is a useful tool for your data analysis and machine learning work. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation and basic usage <a class=\"anchor\" id=\"seccion-2\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation\n",
    "\n",
    "To install ds11mltoolkit, you can use `pip`, the Python package manager. Open a command prompt or terminal and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ds11mltoolkit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download and install the latest version of ds11mltoolkit and its dependencies.\n",
    "\n",
    "If you prefer to install ds11mltoolkit from source, you can clone the repository from GitHub and install it using the setup.py script. First, clone the repository using git:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# git clone https://github.com/..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, navigate to the root directory of the repository and run the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python setup.py install"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "Once you have installed ds11mltoolkit, you can import it in your Python scripts or interactive notebooks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ds11mltoolkit as mlt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ds11mltoolkit imported, you can start using its functions and methods to perform various data analysis, preprocessing, machine learning, and visualization tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Started\n",
    "\n",
    "To get started with ds11mltoolkit, we recommend that you first take a look at the documentation for each of the main functions listed in the table of contents. The documentation provides detailed information on how to use each function and method and includes code examples to help you get started.\n",
    "\n",
    "Here's a brief example of how to load a dataset and perform some basic data analysis tasks using ds11mltoolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ds11mltoolkit as mlt\n",
    "\n",
    "# Load dataset\n",
    "url = \"\"\n",
    "data = mlt.read_url('https://raw.githubusercontent.com/guipsamora/pandas_exercises/master/02_Filtering_%26_Sorting/Euro12/Euro_2012_stats_TEAM.csv')\n",
    "\n",
    "# Basic data analysis\n",
    "print(mlt.list_categorical_columns(data))\n",
    "print(mlt.heatmap(data))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code snippet uses Pandas to load a dataset and then uses ds11mltoolkit's list categorical columns and show heatmap plot, functions to perform some basic data analysis tasks. You can find more examples like this in this documentation for each function.\n",
    "\n",
    "That's it for the installation and usage section. In the following sections, we will dive into the details of each section of ds11mltoolkit and show you how to use its functions and methods to perform various data analysis, preprocessing, machine learning, and visualization tasks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Framework <a class=\"anchor\" id=\"seccion-3\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagrama framework](./assets/diagrama.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top-level directory contains the following files and folders:\n",
    "\n",
    ">\n",
    "> - **test/**: This directory contains the testing files necessary to validate the library's functions.\n",
    "> \n",
    "> - **toolkit/**: This directory contains the core implementation of the library, split across several modules (data_analysis.py, data_processing.py, etc.). The `__init__.py` file serves as the entry point to the library, importing and exposing the functions defined in the modules.\n",
    "> \n",
    "> - **readme.md**: This file provides a brief overview of the library and its usage.\n",
    "> \n",
    "> - **license.txt**: This file specifies the license under which the library is distributed.\n",
    "> \n",
    "> - **requirements.txt**: This file lists the dependencies required by the library.\n",
    "> \n",
    "> - **setup.py**: This file contains the metadata needed to package the library and upload it to the Python Package Index (PyPI).\n",
    "> \n",
    "> - **documentation.ipynb**: This file provides a more detailed description of the library's features and usage, using Jupyter notebooks.\n",
    ">\n",
    "</br>\n",
    "Overall, this structure should make it easy for users to understand and navigate the library's code, and for contributors to add new functionality in a modular way."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions descriptions <a class=\"anchor\" id=\"seccion-3\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis functions <a class=\"anchor\" id=\"seccion-4\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read_url <a class=\"anchor\" id=\"seccion-5\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads a CSV file from a URL using the pandas library in Python. The CSV file is read using different delimiters and encodings to handle different file types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_url(url):\n",
    "\n",
    "    '''\n",
    "    This function reads a CSV file from a URL using the pandas library in Python. \n",
    "    The CSV file is read using different delimiters and encodings to handle different file types.\n",
    "    Parameters\n",
    "    ----------\n",
    "    url : is a valid url.\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "        df : is the dataframe with the data already loaded into memory.\n",
    "    '''\n",
    "\n",
    "    sep=[',', ';', '|', ':','\\t','\\s+']\n",
    "    encoding=['utf-8', 'latin-1', 'latin1', 'iso-8859-1', 'iso8859-1', 'ascii', 'us-ascii', 'utf-16', 'utf16', 'utf-32', 'utf32']\n",
    "    \n",
    "    for s in sep:\n",
    "        for e in encoding:\n",
    "\n",
    "                df = pd.read_csv(url, sep=s, encoding=e)\n",
    "\n",
    "                if df.shape[1] == 1:\n",
    "                    continue\n",
    "                else:\n",
    "                    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read_csv_zip <a class=\"anchor\" id=\"seccion-6\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function reads a CSV file from a zip file and returns its contents as a pandas DataFrame object. The function takes three parameters: the name of the zip file that contains the CSV file, the name of the CSV file to read from the zip file, and an optional separator character to use when parsing the CSV file (default is ';'). The function first opens the zip file and then extracts the CSV file using the given file name. It then reads the contents of the CSV file into a pandas DataFrame using the provided separator and returns the resulting DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_zip(zip_file, csv_file, sep=';'):\n",
    "\n",
    "    import pickle\n",
    "    import zipfile\n",
    "    \"\"\"\n",
    "    Upload a CSV file from a zip file with custom separation.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "         zip_file: The name of the zip file that contains the CSV file.\n",
    "         csv_file: The name of the CSV file to upload.\n",
    "         sep: The separator to use when reading the CSV file. the default value is ';'\n",
    "    Returns:\n",
    "    ----------\n",
    "        A pandas DataFrame object that contains the data from the CSV file.\n",
    "    \"\"\"\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip:\n",
    "        with zip.open(csv_file, 'r') as file:\n",
    "            # Read CSV file with custom separator\n",
    "            df = pd.read_csv(file, sep=sep)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## chi_squared_test <a class=\"anchor\" id=\"seccion-7\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function performs a chi-squared test of independence between two categorical variables. The function takes a DataFrame, an independent variable and a dependent variable as inputs. It returns two values: the chi-squared value obtained in the test and the p-value obtained in the test.\n",
    "\n",
    "The function first creates a contingency table from the data in the input DataFrame by cross-tabulating the independent and dependent variables. It then performs a chi-squared test of independence between the variables using the contingency table. Finally, it returns the chi-squared value and p-value obtained in the test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi_squared_test(df, feature, target):\n",
    "    import scipy.stats as stats\n",
    "    import pandas as pd\n",
    "    \"\"\"\n",
    "    This function performs a chi-squared test of independence between two categorical variables.\n",
    "    Params:\n",
    "    - df: A DataFrame containing the variables of interest.\n",
    "    - feature: The independent variable to be analyzed.\n",
    "    - target: The dependent variable to compare the independent variable with.\n",
    "    Returns:\n",
    "    - chi2: The chi-squared value obtained in the test.\n",
    "    - p: The p-value obtained in the test.\n",
    "    \"\"\"\n",
    "    # Create a contingency table from the independent and dependent variable data.\n",
    "    contingency_table = pd.crosstab(df[feature], df[target])\n",
    "\n",
    "    # Perform a chi-squared test of independence between the variables.\n",
    "    chi2, p, dof, expected = stats.chi2_contingency(contingency_table)\n",
    "\n",
    "    # Return the chi-squared value and p-value obtained in the test.\n",
    "    return chi2, p"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing functions <a class=\"anchor\" id=\"seccion-9\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import cv2 \n",
    "import os\n",
    "from skimage.io import imread\n",
    "import shutil\n",
    "from keras.preprocessing.image import (\n",
    "    ImageDataGenerator,\n",
    "    DataFrameIterator,\n",
    "    DirectoryIterator\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## list_categorical_columns<a class=\"anchor\" id=\"seccion-10\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a DataFrame as input and returns a list of the names of the categorical columns in the DataFrame.\n",
    "\n",
    "The function first initializes an empty list 'features'. It then iterates through each column in the input DataFrame and checks if the data type of the column is an object (i.e., a categorical column). If the data type is an object, it appends the column name to the 'features' list. Finally, it returns the list of categorical column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_categorical_columns(df):\n",
    "    '''\n",
    "    Function that returns a list with the names of the categorical columns of a dataframe.\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : dataframe\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "        features: list of names\n",
    "    '''\n",
    "    features = []\n",
    "\n",
    "    for c in df.columns:\n",
    "        t = str(df[c].dtype)\n",
    "        if \"object\"  in t:\n",
    "            features.append(c)\n",
    "    return features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## last_columns <a class=\"anchor\" id=\"seccion-11\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"last_columndf\" function takes a dataframe and a feature (i.e., a column name) as input and returns a new dataframe with the specified feature moved to the last column position. It does so by first creating a list of all column names in the input dataframe using the \".columns.values\" attribute of the dataframe. The specified feature is then removed from this list using the \"remove\" method and added back to the list using the \"append\" method to move it to the last position. Finally, the original dataframe is indexed using the modified list of column names to obtain the new dataframe with the specified feature in the last column position.\n",
    "\n",
    "In summary, the \"last_columndf\" function rearranges the columns of a dataframe such that the specified feature is moved to the last column position and returns the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_columndf(df,feature):\n",
    "    '''\n",
    "    Function will return the dataframe with the column entered at the last position\n",
    "    ----------\n",
    "    df: dataframe\n",
    "    feature: to move at the last position\n",
    "    Return\n",
    "    ----------\n",
    "    df: dataframe\n",
    "    '''\n",
    "\n",
    "    lista=list(df.columns.values)\n",
    "    lista.remove(feature)\n",
    "    lista.append(feature)\n",
    "    df=df[lista]\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## uniq_value <a class=\"anchor\" id=\"seccion-12\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes a list of values as input and returns a list of unique values. It does so by iterating through the input list and checking if each value is already in a separate empty list called \"unique\". If the value is not already in the \"unique\" list, it is added to the list using the \"extend\" method. The resulting \"unique\" list is then returned as the output of the function.\n",
    "\n",
    "In summary, the \"uniq_value\" function removes duplicate values from a given list and returns only the unique values as a new list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def uniq_value(list_values:list):\n",
    "    '''\n",
    "    Function returning the unique values from a list.\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_values:list\n",
    "    Return\n",
    "    ----------\n",
    "    unique: list of unique values\n",
    "    '''\n",
    "    unique = []\n",
    "    for i in list_values:\n",
    "        if i not in unique:\n",
    "            unique.extend(list_values)\n",
    "    return unique"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_imgs <a class=\"anchor\" id=\"seccion-13\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads a set of images located in a directory and resizes them. The images in the directory should be divided into subdirectories according to their classification (target), which can be binary or categorical. The directory path and the desired image size are introduced as parameters. The function iterates over the subdirectories of the directory given, and within each one, it iterates over each file in it. It reads each image, resizes it, and adds it to a variable (X). It also creates a list with the target values of each image. Finally, it returns a dataframe with the file names and the category they belong to, an array with all the loaded images, and an array with their target values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_imgs(path, im_size:int):\n",
    "    \n",
    "    ''' \n",
    "    Function to load a directory of images and resize them for training a Convolutional Neural Network (CNN) model.\n",
    "    IMPORTANT: Images must be divided into subdirectories according to the target \n",
    "    (e.g. one directory for dog photos and another for cat photos).\n",
    "    It can be used for both binary and categorical classification.\n",
    "    Parameters\n",
    "    ----------\n",
    "    - path: Path where the subdirectories with the images are located.\n",
    "    - im_size: Size to which we want to resize the image (e.g. 32).\n",
    "    Returns\n",
    "    ----------\n",
    "    - df: Dataframe with the names of the images and the category to which they belong (target).\n",
    "    - X_train: Array with the image data loaded after resizing.\n",
    "    - y_train: Array with the target values.\n",
    "    '''    \n",
    "\n",
    "    filenames = []\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Create a dictionary with the target values.\n",
    "    class_names = os.listdir(path)\n",
    "    class_names_label = {class_name:i for i , class_name in enumerate(class_names)}\n",
    "\n",
    "    # Iterate over the subdirectories of the given path.\n",
    "    for subdir, dirs, files in os.walk(path):\n",
    "            for file in files:\n",
    "                # Save the names of the files in a list.\n",
    "                filenames.append(file)\n",
    "                if file [-4:] == '.jpg' or file [-4:] == '.png':\n",
    "                    # Read the image in color.\n",
    "                    image = imread(subdir + '\\\\' + file)\n",
    "                    # Resize the image.\n",
    "                    smallimage = cv2.resize(image, (im_size, im_size)) \n",
    "                    # Save the images in the X variable.\n",
    "                    X.append(smallimage)\n",
    "            \n",
    "                    # Save the target values of each image in a list.\n",
    "                    for i in range(len(class_names_label.keys())):\n",
    "                        if list(class_names_label.keys())[i] in subdir:\n",
    "                            y.append(list(class_names_label.values())[i])\n",
    "    \n",
    "    # Returns:\n",
    "\n",
    "    # 1. Dataframe with the names of the images and the category to which they belong.\n",
    "    df = pd.DataFrame({\n",
    "    'filename': filenames,\n",
    "    'category': y\n",
    "    }) \n",
    "\n",
    "    # 2. Array with the image data loaded after resizing.\n",
    "    X_train = np.array(X)\n",
    "\n",
    "    # 3. Array with the y values (target).\n",
    "    y_train = np.array(y)\n",
    "    \n",
    "    return df, X_train, y_train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## class ImageDataGen(ImageDataGenerator) 3-in-1 functions <a class=\"anchor\" id=\"seccion-14\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`gen_from_array`:\n",
    "Takes data and label arrays and generates new data and label arrays of augmented data by applying random transformations to existing arrays. The size of the generated arrays is determined by the length of the biggest label count or by a specified number.\n",
    "\n",
    "`gen_from_dataframe`:\n",
    "Generates batches of augmented data from a dataframe and stores them in their respective folders. Generates as much data as it needs to balance the dataframe.\n",
    "\n",
    "`gen_from_dataframe`:\n",
    "Takes the path to a directory and generates batches of augmented data and stores them in their respective folders. Generates as much data as it needs to balance the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ImageDataGen(ImageDataGenerator):\n",
    "    def gen_from_array(\n",
    "        self,\n",
    "        x,\n",
    "        y=None,\n",
    "        max_image_count=None,\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        sample_weight=None,\n",
    "        seed=None,\n",
    "        save_to_dir='./aug',\n",
    "        save_prefix='',\n",
    "        save_format='png',\n",
    "        ignore_class_split=False,\n",
    "        subset=None\n",
    "    ):\n",
    "        '''Takes data & label arrays, generates new data & label arrays of \n",
    "        augmented data by applying random transformations to existing arrays.\n",
    "        Args:\n",
    "            x (numpy.ndarray or tuple): Input data. A numpy array of rank 4 or \n",
    "                a tuple. If a tuple, the first element should contain the \n",
    "                images and the second element another numpy array or a list of \n",
    "                numpy arrays that are passed to the output without any \n",
    "                modifications. Can be used to feed the model miscellaneous \n",
    "                data along with the images. In case of grayscale data, the \n",
    "                channels axis of the image array should have value 1, in case \n",
    "                of RGB data, it should have value 3, and in case of RGBA data, \n",
    "                it should have value 4.\n",
    "            y (numpy.ndarray, optional): Labels. Defaults to None.\n",
    "            max_image_count (int, optional): Maximum number of augmented \n",
    "                images to generate for each class. If None, generate enough to \n",
    "                equal the size of the largest class. Defaults to None.\n",
    "            batch_size (int, optional): The size of each batch of augmented images. Defaults to 32.\n",
    "            shuffle (bool, optional): Whether to shuffle the data. Defaults to True.\n",
    "            sample_weight (numpy.ndarray, optional): Sample weights. Defaults to None.\n",
    "            seed (int, optional): Random seed. Defaults to None.\n",
    "            save_to_dir (str, optional): If set, this allows you to specify a \n",
    "                directory to which to save the augmented pictures being \n",
    "                generated (useful for visualizing what you are doing). \n",
    "                Defaults to './aug'.\n",
    "            save_prefix (str, optional): Prefix to use for filenames of saved \n",
    "                pictures (only relevant if `save_to_dir` is set). Defaults to ''.\n",
    "            save_format (str, optional): The format to use for saved pictures, \n",
    "                e.g. \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\", \"tif\", \"jpg\". \n",
    "                Only relevant if `save_to_dir` is set. Defaults to 'png'.\n",
    "            ignore_class_split (bool, optional): Whether to ignore the \n",
    "                difference in number of classes in labels across train and \n",
    "                validation split (useful for non-classification tasks). \n",
    "                Defaults to False.\n",
    "            subset (str, optional): The subset of data to use, either \n",
    "                `\"training\"` or `\"validation\"`, if `validation_split` is set \n",
    "                in `ImageDataGenerator`. Defaults to None.\n",
    "        Returns:\n",
    "            Tuple of Numpy arrays: `(X_gen, y_gen)` A tuple of two numpy \n",
    "                arrays, containing the augmented data and labels, respectively.\n",
    "            `X_gen`: The generated augmented images.\n",
    "            `y_gen`: The corresponding labels for the generated images.\n",
    "        '''\n",
    "        if y is None:\n",
    "            y = np.zeros((x.shape[0],))\n",
    "\n",
    "        unique, counts = np.unique(y, return_counts=True)\n",
    "        class_sizes = dict(zip(unique, counts))\n",
    "\n",
    "        if max_image_count is None:\n",
    "            max_image_count = max(counts)\n",
    "\n",
    "        X_gen = []\n",
    "        y_gen = []\n",
    "\n",
    "        for cls in class_sizes:\n",
    "            if class_sizes[cls] >= max_image_count: continue\n",
    "\n",
    "            samples_needed = max_image_count - class_sizes[cls]\n",
    "            images_generated = 0\n",
    "\n",
    "            generator = super().flow(\n",
    "                x=x[y == cls],\n",
    "                y=y[y == cls],\n",
    "                batch_size=batch_size,\n",
    "                shuffle=shuffle,\n",
    "                sample_weight=sample_weight,\n",
    "                seed=seed,\n",
    "                save_to_dir=save_to_dir,\n",
    "                save_prefix=save_prefix,\n",
    "                save_format=save_format,\n",
    "                ignore_class_split=ignore_class_split,\n",
    "                subset=subset\n",
    "            )\n",
    "\n",
    "            while images_generated < samples_needed:\n",
    "                batch = next(generator)\n",
    "\n",
    "                X_gen.extend(batch[0])\n",
    "                y_gen.extend(batch[1])\n",
    "\n",
    "                images_generated += len(batch[1])\n",
    "\n",
    "        X_gen = np.array(X_gen)\n",
    "        y_gen = np.array(y_gen)\n",
    "\n",
    "        return (X_gen, y_gen)\n",
    "        \n",
    "    def gen_from_dataframe(\n",
    "        self,\n",
    "        dataframe,\n",
    "        max_image_count=None,\n",
    "        directory=None,\n",
    "        x_col='filename',\n",
    "        y_col='class',\n",
    "        weight_col=None,\n",
    "        target_size=(256, 256),\n",
    "        color_mode='rgb',\n",
    "        classes=None,\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        save_to_dir='./aug',\n",
    "        save_prefix='',\n",
    "        save_format='png',\n",
    "        subset=None,\n",
    "        interpolation='nearest',\n",
    "        validate_filenames=True,\n",
    "        **kwargs\n",
    "    ) -> DataFrameIterator:\n",
    "        '''Generates batches of augmented/normalized data from a dataframe and \n",
    "        stores them in their respective folders.\n",
    "        Args:\n",
    "            dataframe: Pandas dataframe containing the filepaths relative to\n",
    "                `directory` (or absolute paths if `directory` is None) of the\n",
    "                images in a string column. It should include other column/s\n",
    "                depending on the `class_mode`:\n",
    "                - if `class_mode` is `\"categorical\"` (default value) it must\n",
    "                    include the `y_col` column with the class/es of each image.\n",
    "                    Values in column can be string/list/tuple if a single class\n",
    "                    or list/tuple if multiple classes.\n",
    "                - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
    "                    the given `y_col` column with class values as strings.\n",
    "                - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should\n",
    "                    contain the columns specified in `y_col`.\n",
    "                - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
    "            max_image_count (int, optional): Maximum number of augmented \n",
    "                images to generate for each class. If None, generate enough to \n",
    "                equal the size of the largest class. Defaults to None.\n",
    "            directory: string, path to the directory to read images from. If\n",
    "                `None`, data in `x_col` column should be absolute paths.\n",
    "            x_col: string, column in `dataframe` that contains the filenames (or\n",
    "                absolute paths if `directory` is `None`).\n",
    "            y_col: string or list, column/s in `dataframe` that has the target data.\n",
    "            weight_col: string, column in `dataframe` that contains the sample\n",
    "                weights. Default: `None`.\n",
    "            target_size: tuple of integers `(height, width)`, default: `(256,\n",
    "                256)`. The dimensions to which all images found will be resized.\n",
    "            color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1 or 3 color\n",
    "                channels.\n",
    "            classes: optional list of classes (e.g. `['dogs', 'cats']`). Default\n",
    "                is None. If not provided, the list of classes will be\n",
    "                automatically inferred from the `y_col`, which will map to the\n",
    "                label indices, will be alphanumeric). The dictionary containing\n",
    "                the mapping from class names to class indices can be obtained via\n",
    "                the attribute `class_indices`.\n",
    "            class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
    "                \"raw\", sparse\" or None. Default: \"categorical\".\n",
    "                Mode for yielding the targets:\n",
    "                - `\"binary\"`: 1D numpy array of binary labels,\n",
    "                - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
    "                    Supports multi-label output.\n",
    "                - `\"input\"`: images identical to input images (mainly used to\n",
    "                    work with autoencoders),\n",
    "                - `\"multi_output\"`: list with the values of the different\n",
    "                    columns,\n",
    "                - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
    "                - `\"sparse\"`: 1D numpy array of integer labels,\n",
    "                - `None`, no targets are returned (the generator will only yield\n",
    "                    batches of image data, which is useful to use in\n",
    "                    `model.predict()`).\n",
    "            batch_size: size of the batches of data (default: 32).\n",
    "            shuffle: whether to shuffle the data (default: True)\n",
    "            seed: optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None). This allows you to\n",
    "                optionally specify a directory to which to save the augmented\n",
    "                pictures being generated (useful for visualizing what you are\n",
    "                doing).\n",
    "            save_prefix: str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
    "                \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n",
    "                \"png\".\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if\n",
    "                the target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "            validate_filenames: Boolean, whether to validate image filenames in\n",
    "                `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
    "                option can lead to speed-up in the execution of this function.\n",
    "                Defaults to `True`.\n",
    "              \n",
    "            **kwargs: legacy arguments for raising deprecation warnings.\n",
    "        Returns:\n",
    "            A `DataFrameIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a numpy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a numpy array of corresponding labels.\n",
    "        '''\n",
    "        generator = super().flow_from_dataframe(\n",
    "            dataframe=dataframe,\n",
    "            directory=directory,\n",
    "            x_col=x_col,\n",
    "            y_col=y_col,\n",
    "            weight_col=weight_col,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            validate_filenames=validate_filenames,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        groups = dataframe.groupby(y_col)\n",
    "        class_count = pd.concat([\n",
    "            groups.get_group(label) for label in dataframe[y_col].unique()\n",
    "        ], axis=0).reset_index(drop=True)\n",
    "\n",
    "        if os.path.isdir(save_to_dir):\n",
    "            shutil.rmtree(save_to_dir)\n",
    "        os.mkdir(save_to_dir)\n",
    "\n",
    "        for label in class_count[y_col].unique():\n",
    "            dir_path = os.path.join(save_to_dir, label)\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "            group = groups.get_group(label)\n",
    "            count = len(group)\n",
    "\n",
    "            if max_image_count is None or max_image_count < count:\n",
    "                max_image_count = count\n",
    "\n",
    "            samples_needed = max_image_count - count\n",
    "            images_generated = 0\n",
    "\n",
    "            generator.dataframe = group\n",
    "            generator.save_to_dir = dir_path\n",
    "            \n",
    "            while images_generated < samples_needed:\n",
    "                next(generator)\n",
    "                images_generated += batch_size\n",
    "\n",
    "        return generator\n",
    "\n",
    "    def gen_from_directory(\n",
    "        self,\n",
    "        directory,\n",
    "        max_image_count=None,\n",
    "        target_size=(256, 256),\n",
    "        color_mode='rgb',\n",
    "        classes=None,\n",
    "        class_mode='categorical',\n",
    "        batch_size=32,\n",
    "        shuffle=True,\n",
    "        seed=None,\n",
    "        save_to_dir='./aug',\n",
    "        save_prefix='',\n",
    "        save_format='png',\n",
    "        follow_links=False,\n",
    "        subset=None,\n",
    "        interpolation='nearest',\n",
    "        keep_aspect_ratio=False\n",
    "    ) -> DirectoryIterator:\n",
    "        '''Takes the path to a directory & generates batches of augmented data \n",
    "        and stores them in their respective folders.\n",
    "        Args:\n",
    "            directory: string, path to the target directory. It should contain\n",
    "                one subdirectory per class. Any PNG, JPG, BMP, PPM or TIF images\n",
    "                inside each of the subdirectories directory tree will be included\n",
    "                in the generator.\n",
    "            max_image_count (int, optional): Maximum number of augmented \n",
    "                images to generate for each class. If None, generate enough to \n",
    "                equal the size of the largest class. Defaults to None.\n",
    "            target_size: Tuple of integers `(height, width)`, defaults to `(256,\n",
    "                256)`. The dimensions to which all images found will be resized.\n",
    "            color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
    "                Whether the images will be converted to have 1, 3, or 4 channels.\n",
    "            classes: Optional list of class subdirectories (e.g. `['dogs',\n",
    "                'cats']`). Default: None. If not provided, the list of classes\n",
    "                will be automatically inferred from the subdirectory\n",
    "                names/structure under `directory`, where each subdirectory will be\n",
    "                treated as a different class (and the order of the classes, which\n",
    "                will map to the label indices, will be alphanumeric). The\n",
    "                dictionary containing the mapping from class names to class\n",
    "                indices can be obtained via the attribute `class_indices`.\n",
    "            class_mode: One of \"categorical\", \"binary\", \"sparse\",\n",
    "                \"input\", or None. Default: \"categorical\".\n",
    "                Determines the type of label arrays that are returned:\n",
    "                - \"categorical\" will be 2D one-hot encoded labels,\n",
    "                - \"binary\" will be 1D binary labels,\n",
    "                    \"sparse\" will be 1D integer labels,\n",
    "                - \"input\" will be images identical\n",
    "                    to input images (mainly used to work with autoencoders).\n",
    "                - If None, no labels are returned\n",
    "                  (the generator will only yield batches of image data,\n",
    "                  which is useful to use with `model.predict_generator()`).\n",
    "                  Please note that in case of class_mode None,\n",
    "                  the data still needs to reside in a subdirectory\n",
    "                  of `directory` for it to work correctly.\n",
    "            batch_size: Size of the batches of data (default: 32).\n",
    "            shuffle: Whether to shuffle the data (default: True) If set to\n",
    "                False, sorts the data in alphanumeric order.\n",
    "            seed: Optional random seed for shuffling and transformations.\n",
    "            save_to_dir: None or str (default: None). This allows you to\n",
    "                optionally specify a directory to which to save the augmented\n",
    "                pictures being generated (useful for visualizing what you are\n",
    "                doing).\n",
    "            save_prefix: Str. Prefix to use for filenames of saved pictures\n",
    "                (only relevant if `save_to_dir` is set).\n",
    "            save_format: one of \"png\", \"jpeg\", \"bmp\", \"pdf\", \"ppm\", \"gif\",\n",
    "                \"tif\", \"jpg\" (only relevant if `save_to_dir` is set). Default:\n",
    "                \"png\".\n",
    "            follow_links: Whether to follow symlinks inside\n",
    "                class subdirectories (default: False).\n",
    "            subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
    "                `validation_split` is set in `ImageDataGenerator`.\n",
    "            interpolation: Interpolation method used to resample the image if\n",
    "                the target size is different from that of the loaded image.\n",
    "                Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
    "                If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
    "                supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
    "                `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
    "                \n",
    "            keep_aspect_ratio: Boolean, whether to resize images to a target\n",
    "                size without aspect ratio distortion. The image is cropped in\n",
    "                the center with target aspect ratio before resizing.\n",
    "        Returns:\n",
    "            A `DirectoryIterator` yielding tuples of `(x, y)`\n",
    "            where `x` is a numpy array containing a batch\n",
    "            of images with shape `(batch_size, *target_size, channels)`\n",
    "            and `y` is a numpy array of corresponding labels.\n",
    "        '''\n",
    "        generator = super().flow_from_directory(\n",
    "            directory=directory,\n",
    "            target_size=target_size,\n",
    "            color_mode=color_mode,\n",
    "            classes=classes,\n",
    "            class_mode=class_mode,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            seed=seed,\n",
    "            save_to_dir=save_to_dir,\n",
    "            save_prefix=save_prefix,\n",
    "            save_format=save_format,\n",
    "            follow_links=follow_links,\n",
    "            subset=subset,\n",
    "            interpolation=interpolation,\n",
    "            keep_aspect_ratio=keep_aspect_ratio\n",
    "        )\n",
    "\n",
    "        groups = generator.class_indices\n",
    "\n",
    "        if os.path.isdir(save_to_dir):\n",
    "            shutil.rmtree(save_to_dir)\n",
    "        os.mkdir(save_to_dir)\n",
    "\n",
    "        for label in groups:\n",
    "            dir_path = os.path.join(save_to_dir, label)\n",
    "            os.mkdir(dir_path)\n",
    "\n",
    "            count = np.count_nonzero(generator.classes == groups[label])\n",
    "\n",
    "            if max_image_count is None or max_image_count < count:\n",
    "                max_image_count = count\n",
    "\n",
    "            samples_needed = max_image_count - count\n",
    "            images_generated = 0\n",
    "\n",
    "            generator.save_to_dir = dir_path\n",
    "\n",
    "            while images_generated < samples_needed:\n",
    "                next(generator)\n",
    "                images_generated += batch_size\n",
    "\n",
    "        return generator"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## clean_text <a class=\"anchor\" id=\"seccion-15\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function cleans a text column of a dataframe to process it before performing Natural Language Processing. To do so, it removes duplicates, social media mentions (@), punctuation marks, converts uppercase to lowercase, removes links, stopwords, and applies a stemmer. The parameters of the function are the dataframe, the name of the column where the text is located, the name of the column where the target to predict is located, and the name to save the dataframe as a new file after cleaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df, column:str, language:str, target:str, filename:str='data_processed.csv'):\n",
    "    \n",
    "    ''' \n",
    "    Function to preprocess and clean a dataframe with text as a preliminary step for Natural Language Processing\n",
    "    Parameters\n",
    "    ----------\n",
    "    - df: Dataframe\n",
    "    - column: The name of the column in which the text is located (str)\n",
    "    - language: The language in which the text is written (str) in ENGLISH (e.g. 'spanish', 'english')\n",
    "    - target: The name of the column in which the target to be predicted is located\n",
    "    - filename: Name for the processed dataframe to be saved\n",
    "    Returns\n",
    "    ----------\n",
    "    - df_processed: Dataframe after cleaning. It contains only the text variable and the target variable\n",
    "    '''\n",
    "    stopwords_lang = set(stopwords.words(language))\n",
    "\n",
    "    # Remove duplicated\n",
    "    df.drop_duplicates(subset = column, inplace=True)\n",
    "\n",
    "    # Remove mentions (@)\n",
    "    df[column] = df[column].str.replace(r'\\s*@\\w+', '', regex=True)\n",
    "\n",
    "    # Remove punctuation marks and convert to lowercase\n",
    "    signos = re.compile(r\"(\\.)|(\\;)|(\\:)|(\\!)|(\\?)|(\\)|(\\@)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])|(\\d+)\")\n",
    "\n",
    "    def signs_tweets(tweet):\n",
    "        return signos.sub('', tweet.lower())\n",
    "\n",
    "    df[column] = df[column].apply(signs_tweets)\n",
    "\n",
    "    # Remove links\n",
    "    def remove_links(df):\n",
    "        return \" \".join(['{link}' if ('http') in word else word for word in df.split()])\n",
    "    \n",
    "    df[column] = df[column].apply(remove_links)\n",
    "\n",
    "    # Remove stopwords\n",
    "    def remove_stopwords(df):\n",
    "        return \" \".join([word for word in df.split() if word not in stopwords_lang])\n",
    "    df[column] = df[column].apply(remove_stopwords)\n",
    "\n",
    "    # Apply Stemmer\n",
    "    stemmer = SnowballStemmer(language)\n",
    "\n",
    "    def def_stemmer(x):\n",
    "        return \" \".join([stemmer.stem(word) for word in x.split()])\n",
    "\n",
    "    df[column] = df[column].apply(def_stemmer)\n",
    "\n",
    "    # Save processed data\n",
    "    df_processed = df[[column, target]]\n",
    "    df_processed.to_csv(filename)\n",
    "\n",
    "    return df_processed"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## replace_convert_numeric <a class=\"anchor\" id=\"seccion-19\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log_transform_numeric <a class=\"anchor\" id=\"seccion-21\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common that we have mixed types of data in a dataset in which some variables might not benefit from log transformation while yes would. This function allows the flexible log transformation of a dataset while ignoring given columns and returning all variables both transformed and not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_transform_data(df: pd.DataFrame, col_ignore: List[str]) -> pd.DataFrame:\n",
    "    '''\n",
    "    Log transform the numeric columns and recombine back with the non-numeric\n",
    "    Option to skip specific numeric columns\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset we want to carry out log transform on\n",
    "    col_ignore : List[str]\n",
    "        A list of numeric column names that we want to skip the log transform for\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    pd.DataFrame with same dimensions as input\n",
    "    '''\n",
    "    df_ignore = df[col_ignore]\n",
    "    df_rest = df.drop(columns = col_ignore)\n",
    "    numeric = df_rest.select_dtypes(include=np.number).apply(np.log1p)\n",
    "    non_numeric = df_rest.select_dtypes(exclude=np.number)\n",
    "    \n",
    "    return pd.concat([numeric, non_numeric, df_ignore], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## add_previous <a class=\"anchor\" id=\"seccion-22\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates n new columns with the data corresponding to the n previous values from a class. For example, it can create columns with the values of the previous days of each stock, or previous sports results, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_previous(df, n, clas, values):\n",
    "    \"\"\"\n",
    "    Add columns to the dataframe with the values of the last n events for each class.\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): The input dataframe.\n",
    "    n (int): The number of previous events to include in the output dataframe.\n",
    "    clas (str): name of the column you want to obtain the previous values of.\n",
    "    values (str): name of the column whose previous values you need\n",
    "    \n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Group the dataframe by the clas column\n",
    "    grouped = df.groupby(clas)\n",
    "    \n",
    "    # Initialize a list to store the shifted values\n",
    "    shifted_values = []\n",
    "    \n",
    "    # Shift the values within each group n times to get the previous values of your clas\n",
    "    for i in range(1, n + 1):\n",
    "        shifted_values.append(grouped[values].shift(i))\n",
    "    \n",
    "    # Concatenate the shifted values with the original dataframe\n",
    "    new_cols = [f'Previous_value-{i}' for i in range(1, n + 1)]\n",
    "    for i, col in enumerate(new_cols):\n",
    "        df[col] = shifted_values[i]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _exponential_smooth <a class=\"anchor\" id=\"seccion-24\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function _exponential_smooth is a Python function that performs exponential smoothing on a given dataset, using an exponentially weighted moving average. Exponential smoothing is a technique that is commonly used in time series analysis to smooth out the fluctuations and noise in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _exponential_smooth(data, alpha):\n",
    "    \"\"\"\n",
    "    Function that exponentially smooths dataset so values are less 'rigid'\n",
    "    :param alpha: weight factor to weight recent values more\n",
    "    \"\"\"\n",
    "\n",
    "    smoothed_data = data.ewm(alpha=alpha).mean()\n",
    "\n",
    "    # Check that the first and last values of the smoothed data are the same as the original data\n",
    "    smoothed_data.iloc[0] = data.iloc[0]\n",
    "    smoothed_data.iloc[-1] = data.iloc[-1]\n",
    "\n",
    "    return smoothed_data "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nan_treatment <a class=\"anchor\" id=\"seccion-25\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the Nan's in the DataFrame into 0 or None depnding de type of the column, it could also drop the Nan's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Nantreatment(data, replace=True, replace_value='None', replace_numeric_with_mean=False):\n",
    "    '''\n",
    "    Function:\n",
    "    -----------\n",
    "    This function works with the Nan's inside of a DataFrame, wich give you diferents option when you try to work with them\n",
    "    Parameters:\n",
    "    -----------\n",
    "    data: Pandas DataFrame\n",
    "        Data that the function is going to analyze \n",
    "    replace: bool\n",
    "        Depends if its True or False, True gives you the Nan replace by a zero or the mean if the column is a number\n",
    "        and None if the column is an object,in case that replace is False, drops all the Nan's in the DataFrame\n",
    "    replace_numeric_with_mean: bool\n",
    "        choose if you want to Nan with 0 or with the mean\n",
    "    \n",
    "    Returns:\n",
    "    -----------\n",
    "    Pandas DataFrame\n",
    "        The function returns a copy of the input DataFrame with NaN values replaced or dropped.\n",
    "    '''\n",
    "\n",
    "\n",
    "    if replace:\n",
    "        if replace_numeric_with_mean:\n",
    "            data = data.fillna(value=data.mean())\n",
    "        else:\n",
    "            for name in data.select_dtypes(include=[np.number]):\n",
    "                data[name] = data[name].fillna(value=0)\n",
    "        for name in data.select_dtypes(include=[object]):\n",
    "            data[name] = data[name].fillna(replace_value)\n",
    "    else:\n",
    "        data = data.dropna()\n",
    "    \n",
    "    return data.reset_index(drop=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## convert_to_numeric <a class=\"anchor\" id=\"seccion-26\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert each column you want to numerical values, ingoring NaN values. In Addition, if you do not want to delete any NaN values yet, this is your function. This make you operate with all your dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_numeric(df,column:str):\n",
    "    '''\n",
    "    This function convert any number string in that column, to int or float ignoring any NaN value.\n",
    "    df -> dataframe we are working with\n",
    "    column -> column which we want to convert to numeric. Must be 'str'\n",
    "    Return:\n",
    "    Dataframe with columns already changed\n",
    "    '''\n",
    "    df[column] = df[column].apply(lambda x: pd.to_numeric(x, errors = 'coerce'))\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## winner_loser <a class=\"anchor\" id=\"seccion-32\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compares the data between odd and even rows of a dataframe and gives Victory-Loss-Draw depending on who scores a bigger punctuation in the selected column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def winner_loser(x, df, column):\n",
    "\n",
    "    \"\"\"\n",
    "    Comparator of odd and even rows, checks which one is a bigger value and returns Victory, Loss or Draw\n",
    "    according to that. Prepared for sports, but appliable to other uses.\n",
    "    Args:\n",
    "    x (int): number of the index\n",
    "    df (df): dataframe to work in\n",
    "    column (str): name of the column we want to compare\n",
    "    Return: Victory, Draw or Loss\n",
    "    \n",
    "    \"\"\"\n",
    "    if (x+2) % 2 == 0:                                  \n",
    "        if df[column][x] > df[column][x+1]:     \n",
    "            x = 'Victory'                               \n",
    "            return x                                    \n",
    "        elif df[column][x] < df[column][x+1]:   \n",
    "            x = 'Loss'\n",
    "            return x\n",
    "        else:\n",
    "            x = 'Draw'\n",
    "            return x\n",
    "    if (x+2) % 2 != 0:                                 \n",
    "        if df[column][x] > df[column][x-1]:\n",
    "            x = 'Victory'\n",
    "            return x\n",
    "        elif df[column][x] < df[column][x-1]:\n",
    "            x = 'Loss'\n",
    "            return x\n",
    "        else:\n",
    "            x = 'Draw'\n",
    "            return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Machine Learning <a class=\"anchor\" id=\"seccion-28\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_curve, auc, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import List, Union\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium import webdriver\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time\n",
    "import io\n",
    "from PIL import Image\n",
    "import pickle\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm_model <a class=\"anchor\" id=\"seccion-55\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function of a standard model of a LSTM neural network. The parameters to introduce are: A tuple that specifies the input data, Number of neurons of the LSTM layer, Number of neurons of the Dense layer and number of classes or values of the output. The output that the function will have is the model that only has to specify the data that you want it to take into account to carry out the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_model(input_shape, lstm_units, dense_units, output_shape):\n",
    "    \"\"\"\n",
    "    Function of a standard LSTM type neural network model. \n",
    "    The output layer has \"sigmoid\" activation so it is remixed in classification applications.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - input_shape: The input shape for the neural network. It is a tuple that specifies the shape of the input data (e.g., (timesteps, features)).\n",
    "        - lstm_units: The number of units in the LSTM layer.\n",
    "        - dense_units: The number of units in the dense layer.\n",
    "        - output_shape: The output shape for the neural network. It is a number that specifies the number of output classes or values (0,1).\n",
    "    Return\n",
    "    ------\n",
    "        - model\n",
    "    \"\"\"\n",
    "    # Define the sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add a LSTM layer with the specified number of units and the input form\n",
    "    model.add(LSTM(units=lstm_units, input_shape=input_shape))\n",
    "\n",
    "    # Add a dense layer with the specified number of units.\n",
    "    model.add(Dense(units=dense_units))\n",
    "\n",
    "    # Add an output layer with the specified shape\n",
    "    model.add(Dense(units=output_shape, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export_model <a class=\"anchor\" id=\"seccion-16\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saves an already trained model to a specified path.\n",
    "- Parameters: model, destination_path, model name and timestamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model(model:object, dir_model:str, name_model:str, timestamp:bool=False):\n",
    "    '''\n",
    "    Function to export a model\n",
    "    Parameters\n",
    "    ----------\n",
    "        model: algorithm / model we want to save.\n",
    "        dir_model: directory to save the model.\n",
    "        name_model: name of the model to save.\n",
    "        \n",
    "        timestamp: time stamp to rename the model.\n",
    "    '''\n",
    "    try:\n",
    "        # Format the current date and time for the renaming of the file to be exported\n",
    "        if timestamp:\n",
    "            now = datetime.now()\n",
    "            year = now.strftime('%Y')[2:]\n",
    "            timestamp = '_' + year + now.strftime('%m%d%H%M%S')\n",
    "        else:\n",
    "            timestamp = ''\n",
    "\n",
    "        # Export the model with the renamed model to the specified directory\n",
    "        filename = os.path.join(dir_model, name_model + timestamp)\n",
    "        with open(filename, 'wb') as archivo:\n",
    "            pickle.dump(model, archivo)\n",
    "\n",
    "        # Show info\n",
    "        print('-'*46)\n",
    "        print('Model saved at', dir_model)\n",
    "        print('-'*46)\n",
    "\n",
    "    except SyntaxError:\n",
    "        print('Fix your syntax')\n",
    "\n",
    "    except TypeError:\n",
    "        print('Oh no! A TypeError has occured')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('A ValueError occured!')\n",
    "\n",
    "    except OSError as err:\n",
    "        print('OS error:', err)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Unexpected {err}, {type(err)}')\n",
    "    \n",
    "    except: \n",
    "        print('Something went wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import_model <a class=\"anchor\" id=\"seccion-17\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports a model from a specified path. \n",
    "- Parameters: source_path and model name. \n",
    "- Returns the saved model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model(dir_model:str, name_model:str):\n",
    "    '''\n",
    "    Function to import a model\n",
    "    Parameters\n",
    "    ----------\n",
    "        dir_model: directory to import the model.\n",
    "        \n",
    "        name_model: name of the model to be imported.\n",
    "    \n",
    "    Return\n",
    "    ------\n",
    "        model_import: algorithm / model we have saved.\n",
    "    '''\n",
    "    try:\n",
    "        # Import the model\n",
    "        filename = os.path.join(dir_model, name_model)\n",
    "        with open(filename, 'rb') as archivo:\n",
    "            model_import = pickle.load(archivo)\n",
    "\n",
    "        return model_import\n",
    "\n",
    "    except SyntaxError:\n",
    "        print('Fix your syntax')\n",
    "\n",
    "    except TypeError:\n",
    "        print('Oh no! A TypeError has occured')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('A ValueError occured!')\n",
    "\n",
    "    except OSError as err:\n",
    "        print('OS error:', err)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Unexpected {err}, {type(err)}')\n",
    "    \n",
    "    except: \n",
    "        print('Something went wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## processing_model_classification <a class=\"anchor\" id=\"seccion-18\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs training and prediction of a classification model. Splits the data into train and test. \n",
    "\n",
    "- Options: percentage split for the test part (test_size_split), shuffle the data before splitting (shuffle_split), assign a seed to the train_test_split (random_state_split). It also allows to perform various scaling and display on screen the scoring results for test and train (we call the show_scoring function). \n",
    "\n",
    "- Parameters: 'model' (model we are going to use), 'x' (data for training), 'y' (target data), 'test_size_split', 'shuffle_split', 'random_state_split', 'minMaxScaler' (allows scaling with MinMaxScaler()), 'minMaxScaler_range' (range for scaling with MinMaxScaler()), 'standardScaler' (allows to scale with StandarScaler()), 'train_score' (allows to display training score) and 'test_score' (allows to display prediction score). \n",
    "\n",
    "- Returns 'X_test' (with the scaling transformations if any) and 'y_pred_test' with the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processing_model_classification(model:object, x, y, test_size_split:float=0.25, shuffle_split:bool=False, random_state_split:int=None, minMaxScaler:bool=False, minMaxScaler_range:tuple=(0,1), standardScaler:bool=False, train_score:bool=False, test_score:bool=True):\n",
    "    '''\n",
    "    Function to train and predict the model with a classification algorithm\n",
    "    Parameters\n",
    "    ----------\n",
    "        model: algorithm / model.\n",
    "        x: {array-like, sparse matrix} of shape (n_samples, n_features).\n",
    "            training data.\n",
    "        y: {array-like, sparse matrix} of shape (n_samples,).\n",
    "            target values.\n",
    "        test_size_split: if float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the test split. \n",
    "            If int, represents the absolute number of test samples. If None, it will be set to 0.25.\n",
    "        shuffle_split: whether or not to shuffle the data before splitting.\n",
    "        minMaxScaler: whether or not to transform features by scaling each feature to a given range.\n",
    "        minMaxScaler_range: if minMaxScaler is True, desired range of transformed data.\n",
    "        standardScaler: whether or not to standardize features by removing the mean and scaling to unit variance.\n",
    "        train_score: compute the score of train.\n",
    "        test_score: compute the score of test.\n",
    "    Return\n",
    "    ------\n",
    "        model, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test\n",
    "    '''\n",
    "    try:\n",
    "        # Split Train and Test\n",
    "        if random_state_split == None:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size_split, shuffle=shuffle_split)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=test_size_split, shuffle=shuffle_split, random_state=random_state_split)\n",
    "\n",
    "        # Number of rows to be processed\n",
    "        print('-'*30)\n",
    "        print('Rows to process for Train:', len(X_train))\n",
    "        print('Rows to process for Test:', len(X_test))\n",
    "        print('-'*30)\n",
    "\n",
    "        # Data scaling - MinMaxScaler()\n",
    "        if minMaxScaler:\n",
    "            X_train = MinMaxScaler(feature_range=minMaxScaler_range).fit_transform(X_train)\n",
    "            X_test = MinMaxScaler(feature_range=minMaxScaler_range).fit_transform(X_test)\n",
    "\n",
    "        # Data scaling - StandardScaler()\n",
    "        if standardScaler:\n",
    "            X_train = StandardScaler().fit_transform(X_train)\n",
    "            X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "        # Training the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Model prediction with Train\n",
    "        y_pred_train = model.predict(X_train)\n",
    "\n",
    "        # Model prediction with Test\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Return the mean accuracy on the given train data and labels\n",
    "        print('SCORE TRAIN:',model.score(X_train, y_train).round(3))\n",
    "\n",
    "        # Compute the score of Train\n",
    "        if train_score:\n",
    "            show_scoring(y_train, y_pred_train, 'TRAIN', 3)\n",
    "\n",
    "        # Compute the score of Test\n",
    "        if test_score:\n",
    "            show_scoring(y_test, y_pred_test, 'TEST', 3)\n",
    "\n",
    "        return model, X_train, X_test, y_train, y_test, y_pred_train, y_pred_test # return model, X_test, y_test, y_pred_test\n",
    "    \n",
    "    except SyntaxError:\n",
    "        print('Fix your syntax')\n",
    "\n",
    "    except TypeError:\n",
    "        print('Oh no! A TypeError has occured')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('A ValueError occured!')\n",
    "\n",
    "    except OSError as err:\n",
    "        print('OS error:', err)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Unexpected {err}, {type(err)}')\n",
    "    \n",
    "    except: \n",
    "        print('Something went wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## worst_params <a class=\"anchor\" id=\"seccion-23\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function was created to obtain the worst params of a gridsearch. Sometimes, it is necessary to train models more than once, and in some computers, that process can take long. With this function we have to input the dictionary with the results of our gridsearch, and we will obtain the worst parameters of that model, and the scoring of it. Then, if we are going to train that model with similar data, we can delete them, and the time to train our model will be slightly shorter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worst_params(gridsearch):\n",
    "    '''\n",
    "    Function to obtain the worst params of a gridsearch. In case we need to train a gridsearch multiple times,\n",
    "    it can be useful to know which parameters are likely to be deleted, in order to make our training faster.\n",
    "    Args:\n",
    "    gridsearch (dict): trained gridsearch.cv_results_\n",
    "    '''\n",
    "    position = list(gridsearch['rank_test_score']).index(gridsearch['rank_test_score'].max())\n",
    "    worst_params = gridsearch['params'][position]\n",
    "    worst_scoring = gridsearch['mean_test_score'][position]\n",
    "\n",
    "    return str(worst_params), worst_scoring"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load_model_zip <a class=\"anchor\" id=\"seccion-20\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function loads a model file from a zip file. It takes two parameters, the name of the zip file where the model file is located and the name of the model file to load. The function returns the model loaded from the file. The function first opens the zip file in read mode and then reads the model file from the zip file and loads it into memory using the pickle library. Finally, the loaded model is returned from the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_zip(zip_file, model_file):\n",
    "    import pickle\n",
    "    import zipfile\n",
    "    \"\"\"\n",
    "    Uploads a model file from a zip file.\n",
    "    Parameters\n",
    "    ----------\n",
    "         zip_file: The name of the zip file where the model file is located.\n",
    "         model_file: The name of the model file to load.\n",
    "    Returns:\n",
    "    ----------\n",
    "         The model loaded from the file.\n",
    "    \"\"\"\n",
    "    # Abre el archivo zip en modo lectura\n",
    "    with zipfile.ZipFile(zip_file, \"r\") as zip:\n",
    "        # Lee el archivo de modelo del zip y lo carga en la memoria\n",
    "        with zip.open(model_file, \"r\") as file:\n",
    "            model = pickle.load(file)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## quickregression <a class=\"anchor\" id=\"seccion-29\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes a Regression Machine Learning problem and train with a previously defined X and y splitted in train and test. Returns the MAE, MAPE, MSE, RMSE and R2 scores.\n",
    "\n",
    "**WARNING**\n",
    "It requires specifically having defined X and y splitted as:\n",
    "- X_train\n",
    "- y_train\n",
    "- X_test\n",
    "- y_test\n",
    "\n",
    "Otherwise, this function will not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quickregression(name):\n",
    "    from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error\n",
    "    \"\"\"\n",
    "    Function to save time when doing Machine Learning models. \n",
    "    It only asks the name of the model to train and returns the scoring.\n",
    "    Parameters\n",
    "    ----------\n",
    "    name = Name of the ML model.\n",
    "           Input Example = LinearRegression\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    MAE, MAPE, MSE, RMSE and R2 Scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Fit of the model in the previously split X_train, y_train\n",
    "    model = name()\n",
    "    model.fit(X_train, y_train)\n",
    "    # Predict of the model with X_test\n",
    "    modpred = model.predict(X_test)\n",
    "    # Scores of the model with y_test and the predict values.\n",
    "    print(\"MAE test:\", mean_absolute_error(y_test, modpred))\n",
    "    print(\"MAPE test:\", mean_absolute_percentage_error(y_test, modpred))\n",
    "    print(\"MSE test:\", mean_squared_error(y_test, modpred))\n",
    "    print(\"RMSE test:\", np.sqrt(mean_squared_error(y_test, modpred)))\n",
    "    return(model.score(X_train, y_train))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ignore_columns_polyfeatures <a class=\"anchor\" id=\"seccion-30\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is somtimes the case that we do not want to create polynomial features for all variables and this is especially true for binary variables or for data that is originally categorical. This allows specific columns to be ignored during the creation of polynomial features and returns the full dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ignore_columns_polyfeatures(X: pd.DataFrame, variables_to_ignore: List[str], n: int) -> pd.DataFrame:\n",
    "    '''\n",
    "    This function takes a dataframe as input and will create n polynomial features for all columns except those specified to ignore\n",
    "    It is intended to be used to ignore binary columns for example and to be included in a Pipeline\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : dataframe\n",
    "        It is the dataset we want to selectively create polynomial features\n",
    "    variables_to_ignore : List[str]\n",
    "        a list of column names to ignore in the polynomial feature creation\n",
    "    n : int\n",
    "        the degree for the polynomial fearture creation\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "        df : Dataframe with the changes made\n",
    "    '''\n",
    "    X_poly_features = X.drop(columns = variables_to_ignore)\n",
    "\n",
    "    X_ignore = X[variables_to_ignore].reset_index(drop = True)\n",
    "\n",
    "    poly = PolynomialFeatures(degree = n)\n",
    "\n",
    "    poly_array = poly.fit_transform(X_poly_features)\n",
    "\n",
    "    poly_features_names = poly.get_feature_names_out(X_poly_features.columns)\n",
    "\n",
    "    X_poly_features = pd.DataFrame(poly_array, columns = poly_features_names)\n",
    "\n",
    "    return pd.concat([X_ignore, X_poly_features], axis = 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## balance_binary_target <a class=\"anchor\" id=\"seccion-33\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function detects the target binary variable, obtains the distribution of it. And, in case of an unbalanced target, it will try different oversampling strategies in order to return a dataframe with a balanced target variable. If visualize = True, it will show a barplot with the new balanced distribution of the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_binary_target(df, strategy='smote', minority_ratio=None, visualize=False):\n",
    "    \"\"\"\n",
    "    This function balances a target binary variable of a dataframe using different oversampling strategies.\n",
    "    Args:\n",
    "    - df: dataframe with the target variable to balance.\n",
    "    - strategy: oversampling strategy to use (default='smote'). The options are: 'smote', 'adasyn' or 'random'.\n",
    "    - minority_ratio: proportion of the minority class after oversampling (default=None).\n",
    "    - visualize: if True, visualize the balanced data (default=False).\n",
    "    Returns:\n",
    "    - DataFrame: dataframe with the balanced target variable.\n",
    "    \"\"\"\n",
    "\n",
    "    # Automatically detect the target variable column.\n",
    "    target_col = df.select_dtypes(include=['bool', 'int', 'float']).columns[0]\n",
    "\n",
    "    # Separate target variable and predictor variables\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Oversampling the minority class using the selected strategy\n",
    "    if strategy == 'smote':\n",
    "        sampler = SMOTE(random_state=42)\n",
    "    elif strategy == 'adasyn':\n",
    "        sampler = ADASYN(random_state=42)\n",
    "    elif strategy == 'random':\n",
    "        sampler = RandomOverSampler(random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Estrategia de sobremuestreo invlida. Las opciones son: 'smote', 'adasyn' o 'random'.\")\n",
    "\n",
    "    X_resampled, y_resampled = sampler.fit_resample(X, y)\n",
    "\n",
    "    # Adjust the minority class ratio if specified\n",
    "    if minority_ratio is not None:\n",
    "        target_counts = y_resampled.value_counts()\n",
    "        minority_class = target_counts.idxmin()\n",
    "        majority_class = target_counts.idxmax()\n",
    "\n",
    "        minority_count = target_counts[minority_class]\n",
    "        majority_count = target_counts[majority_class]\n",
    "\n",
    "        desired_minority_count = int(minority_ratio * (minority_count + majority_count))\n",
    "\n",
    "        if desired_minority_count < minority_count:\n",
    "            drop_indices = y_resampled[y_resampled == minority_class].index[:minority_count - desired_minority_count]\n",
    "            X_resampled = X_resampled.drop(drop_indices)\n",
    "            y_resampled = y_resampled.drop(drop_indices)\n",
    "        elif desired_minority_count > minority_count:\n",
    "            extra_count = desired_minority_count - minority_count\n",
    "            extra_X, extra_y = sampler.fit_resample(X_resampled[y_resampled == minority_class], y_resampled[y_resampled == minority_class])\n",
    "            X_resampled = pd.concat([X_resampled, extra_X], axis=0)\n",
    "            y_resampled = pd.concat([y_resampled, extra_y], axis=0)\n",
    "\n",
    "    # Display the balanced data if specified\n",
    "    if visualize:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "        ax.set_title('Distribucin de la variable objetivo balanceada')\n",
    "        y_resampled.value_counts().plot(kind='bar', ax=ax)\n",
    "        ax.set_xlabel(target_col)\n",
    "        ax.set_ylabel('Frecuencia')\n",
    "\n",
    "    # Combine the predictor variables and the balanced target variable in a new dataframe.\n",
    "    df_resampled = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "\n",
    "    return df_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## image_scrap <a class=\"anchor\" id=\"seccion-34\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download from a chosen link as many images from Google Chrome as you need. It is necessary to install a driver in Chrome, and name it 'chromedriver'. It returns the pictures to a new folder that the function creates and is called \"my_images\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_scrap(url, n:int):\n",
    "\t'''\n",
    "\tFunction to scrap chrome images and get n images we want, and it create a new folder as 'my_images'.\n",
    "\tAs we know, we are using selenium, we will need a driver in Chrome.\n",
    "\tMust have driver from Chrome to run it [chrome](https://chromedriver.chromium.org/), file name = 'chromedriver' and dowload in the same path as the scrip or jupyter. \n",
    "\tParameters\n",
    "\t----------\n",
    "\turl -> chrome images web link, must be all way long.\n",
    "\tn -> number of images you want to have in the folder. Must be 'int'\n",
    "\t\n",
    "\tReturn\n",
    "\t----------\n",
    "\tFolder called 'my_images' with n images, where you can show as much time as you want\n",
    "\t\n",
    "\t'''\n",
    "\tcurrent_dir = os.getcwd()\n",
    "\tdriver_path = os.path.join(current_dir, \"chromedriver.exe\")\n",
    "\n",
    "\twd = webdriver.Chrome(driver_path)\n",
    "\n",
    "\tdef get_images_from_google(url, wd, delay, max_images):\n",
    "\t\tdef scroll_down(wd):\n",
    "\t\t\twd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\t\t\ttime.sleep(delay)\n",
    "\n",
    "\t\turl = url\n",
    "\t\twd.get(url)\n",
    "\n",
    "\t\tloadMore = wd.find_element(By.XPATH, '/html/body/c-wiz/div/div/div/div[2]/div/div[3]/div/div/form/div/div/button').click()\n",
    "\n",
    "\t\timage_urls = set()\n",
    "\t\tskips = 0\n",
    "\n",
    "\t\twhile len(image_urls) + skips < max_images:\n",
    "\t\t\tscroll_down(wd)\n",
    "\n",
    "\t\t\tthumbnails = wd.find_elements(By.CLASS_NAME, \"Q4LuWd\")\n",
    "\n",
    "\t\t\tfor img in thumbnails[len(image_urls) + skips:max_images]:\n",
    "\t\t\t\ttry:\n",
    "\t\t\t\t\timg.click()\n",
    "\t\t\t\t\ttime.sleep(delay)\n",
    "\t\t\t\texcept:\n",
    "\t\t\t\t\tcontinue\n",
    "\n",
    "\t\t\t\timages = wd.find_elements(By.CLASS_NAME, \"n3VNCb\")\n",
    "\t\t\t\tfor image in images:\n",
    "\t\t\t\t\tif image.get_attribute('src') in image_urls:\n",
    "\t\t\t\t\t\tmax_images += 1\n",
    "\t\t\t\t\t\tskips += 1\n",
    "\t\t\t\t\t\tbreak\n",
    "\n",
    "\t\t\t\t\tif image.get_attribute('src') and 'http' in image.get_attribute('src'):\n",
    "\t\t\t\t\t\timage_urls.add(image.get_attribute('src'))\n",
    "\t\t\t\t\t\tprint(f\"Found {len(image_urls)}\")\n",
    "\n",
    "\t\treturn image_urls\n",
    "\n",
    "\n",
    "\tdef download_image(download_path, url, file_name):\n",
    "\t\ttry:\n",
    "\t\t\timage_content = requests.get(url).content\n",
    "\t\t\timage_file = io.BytesIO(image_content)\n",
    "\t\t\timage = Image.open(image_file)\n",
    "\t\t\tfile_path = download_path + file_name\n",
    "\n",
    "\t\t\twith open(file_path, \"wb\") as f:\n",
    "\t\t\t\timage.save(f, \"JPEG\")\n",
    "\n",
    "\t\t\tprint(\"Success\")\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint('FAILED -', e)\n",
    "\n",
    "\n",
    "\turls = get_images_from_google(url,wd, 1, n)\n",
    "\t\n",
    "\t\n",
    "\tcurrent_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "\n",
    "\tdownload_dir = os.path.join(current_dir, \"my_images\")\n",
    "\t\n",
    "\n",
    "\tif not os.path.exists(download_dir):\n",
    "\t\tos.makedirs(download_dir)\n",
    "\n",
    "\tfor i, url in enumerate(urls):\n",
    "\t\t\tdownload_image(download_dir, url, str(i) + \".jpg\")\n",
    "\n",
    "\twd.quit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create_multiclass_prediction_df <a class=\"anchor\" id=\"seccion-36\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating a multiclass classfication model, we want to be able assess the predictions for each record across all potential classes. This function will carry out the predictions and return a dataframe with the probabilities for each class alongside the real label and the most likely predicted label for use in plotting or analysis downstream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multiclass_prediction_df(model, class_names: List[str], X_test: Union[pd.DataFrame, np.ndarray], y_test: Union[list, np.ndarray], only_wrong: bool = False) -> pd.DataFrame:\n",
    "    '''\n",
    "    This will generate a dataframe from the predictions of a model on X_test for easy analysis of model performance\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras model\n",
    "        It is the dataset we want to selectively create polynomial features\n",
    "    class_names : List[str]\n",
    "        a list of the class names which must be in order that relates to the numbers in y_test\n",
    "    X_test : pd.DataFrame or np.ndarray\n",
    "        The data for X_test\n",
    "    y_test : list or np.ndarray\n",
    "        The y_test with the numeric values of the class names\n",
    "    only_wrong : bool\n",
    "        defaults to False but if set to True, only incorrect predictions will be kept\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "        df : Dataframe containing the predictions from model.predict along with the top prediction and actual labels\n",
    "    \n",
    "    '''\n",
    "    # make the predictions with model on X_test\n",
    "    model_predictions_df = round(pd.DataFrame(model.predict(X_test)), 2)\n",
    "    # assign class label names as columns\n",
    "    model_predictions_df.columns = class_names\n",
    "    # create top prediction column\n",
    "    model_predictions_df['Top Prediction'] = model_predictions_df.apply(lambda x: x.idxmax(), axis = 1)\n",
    "    # create class label column\n",
    "    model_predictions_df['Label'] = [model_predictions_df.columns[y_class] for y_class in y_test]\n",
    "    # filter for model errors if \"only_wrong == True\"\n",
    "    if only_wrong:\n",
    "        model_predictions_df = model_predictions_df[model_predictions_df['Top Prediction'] != model_predictions_df['Label']]\n",
    "    # return the dataframe\n",
    "    return model_predictions_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## show_scoring <a class=\"anchor\" id=\"seccion-37\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculates the scoring of a Classifier model prediction. It takes y (target values), and y_prediction (prediction values), and prints the metrics accuracy_score, roc_curve.auc, roc_auc_score and confusion matrix, and returns a dictionary with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_scoring(y, y_prediction, label:str, round:int=3, auc_sc:bool=True, roc_auc_sc:bool=True, confusion_matrix_sc:bool=True):\n",
    "    '''\n",
    "    Function to calculate a prediction score\n",
    "    Parameters\n",
    "    ----------\n",
    "        y: target values.\n",
    "        \n",
    "        y_prediction: prediction values.\n",
    "        label: value type label.\n",
    "        round: value rounding.\n",
    "        auc_sc: compute Area Under the Curve (AUC) using the trapezoidal rule.\n",
    "        roc_auc_sc: compute Area Under the Receiver Operating Characteristic Curve (ROC AUC) from prediction scores.\n",
    "        confusion_matrix_sc: compute confusion matrix to evaluate the accuracy of a classification.\n",
    "    Return\n",
    "    ------\n",
    "        dict(accu --> accuracy_score, \n",
    "            auc_s --> roc_curve.auc, \n",
    "            roc_auc --> roc_auc_score, \n",
    "            conf_mat --> confusion_matrix)\n",
    "    '''\n",
    "    try:\n",
    "        auc_r = None\n",
    "        roc_auc_r = None\n",
    "        conf_mat_r = None\n",
    "\n",
    "        print('-'*30)\n",
    "\n",
    "        accu_r = accuracy_score(y, y_prediction).round(round)\n",
    "        print('ACCURACY',label,':',accu_r)\n",
    "\n",
    "        if auc_sc:\n",
    "            fpr, tpr, thresh = roc_curve(y, y_prediction)\n",
    "            auc_r = auc(fpr, tpr).round(round)\n",
    "            print('AUC',label,':',auc_r)\n",
    "\n",
    "        if roc_auc_sc:\n",
    "            roc_auc_r = roc_auc_score(y, y_prediction).round(round)\n",
    "            print('ROC AUC',label,':',roc_auc_r)\n",
    "        \n",
    "        if confusion_matrix_sc:\n",
    "            c_mat = confusion_matrix(y, y_prediction, normalize='true')\n",
    "            conf_mat_r = c_mat.round(round)\n",
    "            print('CONFUSION MATRIX',label,':',conf_mat_r)\n",
    "\n",
    "        print('-'*30)\n",
    "\n",
    "        return dict(accu_r=accu_r, auc_r=auc_r, roc_auc_r=roc_auc_r, conf_mat_r=conf_mat_r)\n",
    "\n",
    "    except SyntaxError:\n",
    "        print('Fix your syntax')\n",
    "\n",
    "    except TypeError:\n",
    "        print('Oh no! A TypeError has occured')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('A ValueError occured!')\n",
    "\n",
    "    except OSError as err:\n",
    "        print('OS error:', err)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Unexpected {err}, {type(err)}')\n",
    "    \n",
    "    except: \n",
    "        print('Something went wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## predict_model_classification <a class=\"anchor\" id=\"seccion-38\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performs the prediction of a scoring model. \n",
    "- Options: perform several scalings and display the scoring results on the screen (we call the show_scoring function). \n",
    "- Parameters: 'model' (model to use), 'X_test' (sample data to predict), 'y_test' (target data), 'minMaxScaler' (allows scaling with MinMaxScaler()), 'minMaxScaler_range' (range to scale with MinMaxScaler() ), 'standardScaler' (allows scaling with StandarScaler()) and 'test_score' (allows displaying the prediction scoring). \n",
    "- Returns: 'X_test' (with the scaling transformations if any) and 'y_pred_test' with the model prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_model_classification(model:object, X_test, y_test, minMaxScaler:bool=False, minMaxScaler_range:tuple=(0,1), standardScaler:bool=False, test_score:bool=True):\n",
    "    '''\n",
    "    Function to predict the model with a classification algorithm\n",
    "    Parameters\n",
    "    ----------\n",
    "        model: algorithm / model.\n",
    "        X_test: {array-like, sparse matrix} of shape (n_samples, n_features).\n",
    "            training data.\n",
    "        y_test: {array-like, sparse matrix} of shape (n_samples,).\n",
    "            target values.\n",
    "        minMaxScaler: whether or not to transform features by scaling each feature to a given range.\n",
    "        minMaxScaler_range: if minMaxScaler is True, desired range of transformed data.\n",
    "        standardScaler: whether or not to standardize features by removing the mean and scaling to unit variance.\n",
    "        test_score: calculates the score of test\n",
    "        \n",
    "    Return\n",
    "    ------\n",
    "        X_test, y_pred_test\n",
    "    '''\n",
    "    try:\n",
    "        # Data scaling - MinMaxScaler()\n",
    "        if minMaxScaler:\n",
    "            X_test = MinMaxScaler(feature_range=minMaxScaler_range).fit_transform(X_test)\n",
    "\n",
    "        # Data scaling - StandardScaler()\n",
    "        if standardScaler:\n",
    "            X_test = StandardScaler().fit_transform(X_test)\n",
    "\n",
    "        # Model prediction with Test\n",
    "        y_pred_test = model.predict(X_test)\n",
    "\n",
    "        # Compute the score of Test\n",
    "        if test_score:\n",
    "            show_scoring(y_test, y_pred_test, 'TEST', 3)\n",
    "\n",
    "        return X_test, y_pred_test\n",
    "    \n",
    "    except SyntaxError:\n",
    "        print('Fix your syntax')\n",
    "\n",
    "    except TypeError:\n",
    "        print('Oh no! A TypeError has occured')\n",
    "        \n",
    "    except ValueError:\n",
    "        print('A ValueError occured!')\n",
    "\n",
    "    except OSError as err:\n",
    "        print('OS error:', err)\n",
    "\n",
    "    except Exception as err:\n",
    "        print(f'Unexpected {err}, {type(err)}')\n",
    "    \n",
    "    except: \n",
    "        print('Something went wrong')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnsupervisedKMeans <a class=\"anchor\" id=\"seccion-39\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works with the unsupervised model of Kmeans. It shows how depending the number of clusters that you want the inertia and the silhouette score are going to go up or down to facilitate your choose of k, and also have the model of Kmeans to see those clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnsupervisedCluster(df,motive='analisys', Range=20,k=3):\n",
    "\n",
    "\n",
    "    '''\n",
    "    \n",
    "    Function:\n",
    "    -----------\n",
    "    This function works with the unsupervised model of Kmeans, and its objective is to show you how depending the number of \n",
    "    clusters that you want the inertia and the silhouette score are going to go up or down to facilitate your choose oof k, and also\n",
    "    have the model of Kmeans to see thoose clusters.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: Pandas DataFrame\n",
    "        Data that the function is going to analyze \n",
    "    motive: str\n",
    "        Depend in wich word you use the function is going to ralize different things, for example 'Analysis' show you 2 graphs \n",
    "        and 'clustering' give you in wich cluster is every target\n",
    "    Range: int\n",
    "        Range of k's that are in the graph showing the inertia and the silhouette score for each one of them\n",
    "    K: int\n",
    "        number that indicates how much clusters do you want in the modeling of Kmeans\n",
    "    Returns:\n",
    "    -----------\n",
    "    Pandas DataFrame\n",
    "        The function returns a dataframe with an aditional column wich have in wich cluster each target is in\n",
    "    '''\n",
    "\n",
    "    if motive=='analisys':\n",
    "        km_list = [KMeans(n_clusters=a, random_state=42).fit(df) for a in range(2,Range)]\n",
    "        inertias = [model.inertia_ for model in km_list]\n",
    "        silhouette_score_list = [silhouette_score(df, model.labels_) for model in km_list]\n",
    "\n",
    "        plt.figure(figsize=(20,5))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        sns.set(rc={'figure.figsize':(10,10)})\n",
    "        plt.plot(range(2,Range), inertias)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(\"inertias\")\n",
    "        sns.despine()\n",
    "\n",
    "        plt.subplot(122)\n",
    "        sns.set(rc={'figure.figsize':(10,10)})\n",
    "        plt.plot(range(2,Range), silhouette_score_list)\n",
    "        plt.xlabel('k')\n",
    "        plt.ylabel(\"silhouette_score\")\n",
    "        sns.despine()\n",
    "\n",
    "    if motive =='clustering':\n",
    "        kmeans = KMeans(n_clusters=k,n_init=10, random_state=42).fit(df)\n",
    "        df_clusters = pd.DataFrame(kmeans.labels_, columns=['Cluster'])\n",
    "        return df_clusters"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnsupervisedPCA <a class=\"anchor\" id=\"seccion-40\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function works with the unsupervised model of PCA, and its objective is to give you de minimum principal components to satisfy the acumulative variance of your choice, you also can choose the number of PC's that the model is going to work with.\n",
    "Its recomendable that the Nc is a number near to the number of varibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UnsupervisedDR(df, Acumulative_variance=0.85):\n",
    "    '''\n",
    "    Function:\n",
    "    -----------\n",
    "    This function works with the unsupervised model of PCA, and its objective is to give you de minimun\n",
    "    principal components to satisfy the acumulative variance of your choice, you also can choose the number \n",
    "    of PC's that the model is going to work with.\n",
    "    Its recomendable that the Nc is a number near to the number of varibles\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df: Pandas DataFrame\n",
    "        Data that the function is going to analyze \n",
    "    Acumulative_variance: float, default=0.85\n",
    "        Number that indicates how much you want to keep from the original principal components. \n",
    "        Should be a value between 0.0 and 1.0.\n",
    "    Returns:\n",
    "    -----------\n",
    "    Pandas DataFrame\n",
    "        The function returns the original data set with the values changed because of the dimensional reduction.\n",
    "    '''\n",
    "    pca = make_pipeline(StandardScaler(), PCA(n_components=len(df.columns)))\n",
    "    pca.fit(df)\n",
    "    pca_model = pca.named_steps['pca']\n",
    "    variance_ratio_cumsum = np.cumsum(pca_model.explained_variance_ratio_)\n",
    "    min_var_explained = Acumulative_variance\n",
    "    min_num_components = 0\n",
    "    for i, variance_ratio in enumerate(variance_ratio_cumsum):\n",
    "        if (variance_ratio < min_var_explained).any():\n",
    "            min_num_components += 1\n",
    "        else:\n",
    "            break\n",
    "    min_num_components += 1\n",
    "    columnas = ['PC{}'.format(i+1) for i in range(min_num_components)]\n",
    "    pca_pipe = make_pipeline(StandardScaler(), PCA(n_components=min_num_components))\n",
    "    modelo_pca = pca_pipe['pca']\n",
    "    proyecciones = pca_pipe.fit_transform(X=df)\n",
    "    proyecciones = pd.DataFrame(\n",
    "        proyecciones,\n",
    "        columns = columnas,\n",
    "        index   = df.index\n",
    "    )\n",
    "    proyecciones = np.dot(modelo_pca.components_, scale(df).T)\n",
    "    proyecciones = pd.DataFrame(proyecciones, index =columnas)\n",
    "    proyecciones = proyecciones.transpose().set_index(df.index)\n",
    "    modelo_pca = pca_pipe['pca']\n",
    "    reconstruccion = pca_pipe.inverse_transform(proyecciones)\n",
    "    reconstruccion = pd.DataFrame(\n",
    "        reconstruccion,\n",
    "        columns = df.columns,\n",
    "    ).set_index(df.index)\n",
    "    return reconstruccion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funciones de Plot <a class=\"anchor\" id=\"seccion-41\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Union\n",
    "import plotly.graph_objs as go\n",
    "from plotly import subplots\n",
    "from collections import defaultdict\n",
    "import plotly.offline as py\n",
    "from wordcloud import STOPWORDS\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import auc, roc_curve"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## heatmap <a class=\"anchor\" id=\"seccion-42\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a heatmap with all columns you want to correlate with, the columns have been selected before. Ideally for datasets with a lot of features. This make your life easier by just selecting the columns with the highest correlations with the target for checking for correlation with the target and for correlation between potential variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(df, n:int,target:str,columns:None):\n",
    "    '''\n",
    "    Heatmap which show us teh correlation of our numerical column of dataset with the target, where you can add specifics numbers \n",
    "    \n",
    "    df -> must be the dataset we are working with\n",
    "    n -> number of columns we want to correlate with the target\n",
    "    target -> name of the column of the target, must be 'str'\n",
    "    columns -> must be all the columns we have in the dataset in previous step, in type object (df.columns)\n",
    "\n",
    "    Return:\n",
    "    Heatmap with YlOrBr colour and two decimals, only wiht n number of columns which correlate with our target\n",
    "    \n",
    "    '''\n",
    "\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    \n",
    "    cols = df[columns].corr().nlargest(n,target)[target].index\n",
    "\n",
    "    cm = np.corrcoef(df[cols].values.T) \n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    hm = sns.heatmap(cm, cbar=True, annot=True, cmap='YlOrBr', fmt='.2f', yticklabels=cols.values, xticklabels=cols.values)\n",
    "    return hm"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sunburst <a class=\"anchor\" id=\"seccion-43\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get pie Sunburst that show us two columns inside the chart, the first one; the interior is perfect for features which have a bit of uniques values, and the second one; the exterior is perfect for features which have more values, but never more than 20 uniques values. Ideally for showing the characteristics of some variable quickly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sunburst(df, interior:str, exterior:str, col_num:str, title:str):\n",
    "    '''\n",
    "    This is a Plotly Graph similar to pie chart but with two levels, interior is for columns which have one or two unique values, and \n",
    "    the exterior is for columns which have more values.\n",
    "\n",
    "    Parameters\n",
    "\t----------\n",
    "    df -> dataframe we are working with \n",
    "    interior -> recommended for columns which have two or so uniques values. Must be 'str'\n",
    "    exterior -> recommended for columns which have more values, because the graph has more space than inside. Must be 'str'\n",
    "    col_num -> it,s the column which we want measured, show us the quantity of each value for both column (interior and exterior), must be 'str'\n",
    "    title -> the title we want to show in the pie, must be 'str'\n",
    "\n",
    "    Return\n",
    "\t----------\n",
    "\n",
    "    Return a pie chart with two levels, interior and exterior.\n",
    "    \n",
    "    '''\n",
    "\n",
    "    fig = go.Figure()\n",
    "    fig = px.sunburst(df, path=[interior, exterior], values=col_num, template = 'plotly_dark')\n",
    "    fig.update_layout(width=800, height=600, title = title)\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correl_map_max <a class=\"anchor\" id=\"seccion-44\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that, given a DataFrame, eliminates correlations greater than 0.9 since if there is a correlation close to 1 it is detrimental to use it in a Machine Learning model since it will not interpret well and therefore it will not be able to adjust to new data. It also paints a heatmap with those correlations. It is only necessary to enter the dataframe with the columns of the numerical data that you want to take into account for the realization of the heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correl_map_max(dataframe):\n",
    "    \"\"\"\n",
    "    Function that, given a dataframe, eliminates the correlations greater than 0.9, visualizes the correlations and returns a new dataframe with columns that meet the condition of being less than 0.9. \n",
    "    and returns a new dataframe with the columns that meet the condition of being less than 0.9. \n",
    "    Parameters\n",
    "    ----------\n",
    "        - DataFrame: set of the data to which you want to apply.\n",
    "    Returns\n",
    "    -------\n",
    "        - dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = dataframe.corr()\n",
    "\n",
    "    # Eliminate variables with correlation higher than 0.9\n",
    "    high_corr_vars = set()\n",
    "    for i in range(len(corr_matrix.columns)):\n",
    "        for j in range(i):\n",
    "            if abs(corr_matrix.iloc[i, j]) > 0.9:\n",
    "                varname_i = corr_matrix.columns[i]\n",
    "                varname_j = corr_matrix.columns[j]\n",
    "                if corr_matrix[varname_i].std() < corr_matrix[varname_j].std():\n",
    "                    high_corr_vars.add(varname_i)\n",
    "                else:\n",
    "                    high_corr_vars.add(varname_j)\n",
    "    dataframe = dataframe.drop(high_corr_vars, axis=1)\n",
    "\n",
    "    # Generate the visualization of the correlation map\n",
    "    sns.set(style=\"white\")\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corr_matrix, cmap=cmap, vmax=.3, center=0,\n",
    "                square=True, annot=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.show()\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_map <a class=\"anchor\" id=\"seccion-53\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that, given a data frame with latitude and longitude coordinates, represents them on a geographic map, placing the points anywhere on the planet. The parameters to enter are the DataFrame, Latitude column name, Longitude column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map(df, lat_col, lon_col, tooltip_col=None, zoom_start=3, map_type='OpenStreetMap'):\n",
    "    \"\"\"\n",
    "    Function that creates an interactive map using folium from a dataframe with coordinates.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        - df: dataframe with coordinates.\n",
    "        - lat_col: name of the column containing latitudes.\n",
    "        - lon_col: name of the column containing the longitudes.\n",
    "        - tooltip_col: (optional) name of the column containing the additional information to show in the tooltip of each marker.\n",
    "        - zoom_start: (optional) initial zoom level of the map.\n",
    "        - map_type: (optional) type of map to use. Possible values: 'OpenStreetMap', 'Stamen Terrain', 'Stamen Toner', 'Stamen Watercolor', 'CartoDB positron', 'CartoDB dark_matter'.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "        - map: folium Map object with the added markers\n",
    "    \"\"\"\n",
    "    import folium\n",
    "    # Create the map with the indicated type and zoom level.\n",
    "    map = folium.Map(location=[df[lat_col][0], df[lon_col][0]], zoom_start=zoom_start, tiles=map_type)\n",
    "\n",
    "    # Add markers for each point of the dataframe\n",
    "    for index, row in df.iterrows():\n",
    "        location = [row[lat_col], row[lon_col]]\n",
    "        tooltip = row[tooltip_col] if tooltip_col else None\n",
    "        folium.Marker(location=location, tooltip=tooltip).add_to(map)\n",
    "\n",
    "    # Return the map\n",
    "    return map"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_ngram <a class=\"anchor\" id=\"seccion-45\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allows visualizing the frequency of occurrence of words or sets of words in a text depending on the target to predict, as a useful step in Natural Language Processing. The parameters of the function are the dataframe where the text is located, the name of the target column, the name of the text column, and the number of consecutive words that each set will have (2 words, 3 words...). It creates two subplots, one for each of the target values, with an horizontal bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ngrams(df, target:str, text:str, n_gram:int):\n",
    "    ''' \n",
    "    This function allows visualizing the frequency of occurrence of words and n-grams (sets of a number of consecutive words) based on the target variable.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    - df: Dataframe with a text variable.\n",
    "    - target: Column of the dataframe where the target variable is located.\n",
    "    - text: Column of the dataframe where the text is located.\n",
    "    - n_gram: Number of consecutive words whose frequency we want to visualize.\n",
    "    \n",
    "    Returns\n",
    "    ----------\n",
    "    - Bar chart representing the frequency of words, sorted from highest to lowest, divided by target variable.\n",
    "    '''\n",
    "    df1 = df[df[target] ==1]\n",
    "    df0 = df[df[target] ==0]\n",
    "\n",
    "    # Custom function for ngram generation \n",
    "    def generate_ngrams(text, n_gram):\n",
    "        token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n",
    "        ngrams = zip(*[token[i:] for i in range(n_gram)])\n",
    "        return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "    # Custom function for horizontal bar chart \n",
    "    def horizontal_bar_chart(df, color):\n",
    "        trace = go.Bar(\n",
    "            y=df[\"word\"].values[::-1],\n",
    "            x=df[\"wordcount\"].values[::-1],\n",
    "            showlegend=False,\n",
    "            orientation = 'h',\n",
    "            marker=dict(\n",
    "                color=color,\n",
    "            ),\n",
    "        )\n",
    "        return trace\n",
    "\n",
    "    # Get the bar chart from text with label 0 \n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in df0[text]:\n",
    "        for word in generate_ngrams(sent, n_gram):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "    # Get the bar chart from text with label 1 \n",
    "    freq_dict = defaultdict(int)\n",
    "    for sent in df1[text]:\n",
    "        for word in generate_ngrams(sent, n_gram):\n",
    "            freq_dict[word] += 1\n",
    "    fd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\n",
    "    fd_sorted.columns = [\"word\", \"wordcount\"]\n",
    "    trace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n",
    "\n",
    "    # Creating two subplots\n",
    "    fig = subplots.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n",
    "                          subplot_titles=[\"Frequent words from text with label 0\", \n",
    "                                          \"Frequent words from text with label 1\"])\n",
    "    fig.add_trace(trace0, 1, 1)\n",
    "    fig.add_trace(trace1, 1, 2)\n",
    "    fig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\n",
    "    py.iplot(fig, filename='word-plots')\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wordcloudviz <a class=\"anchor\" id=\"seccion-46\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creates a wordcloud visualization from a dataframe column. This is useful to visualise both the total number of words and to quickly reveal which words are most common."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordcloudviz(column):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from wordcloud import WordCloud\n",
    "    \"\"\"\n",
    "    Function to create a quick visualization of wordclouds in a given column of a dataframe called df.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    column = name of the column of the dataframe. \n",
    "             Input example: df['column_name']\n",
    "\n",
    "    Return\n",
    "    ---------\n",
    "        A wordcloud visualization of the words in the column.\n",
    "    \"\"\"\n",
    "    # First, it concatenates the text in a \"single\" text.\n",
    "    text = \" \".join(comment for comment in column)\n",
    "\n",
    "    # Creates a wordcloud visualization\n",
    "    wordcloud = WordCloud(width=800, height=800, background_color='white').generate(text)\n",
    "\n",
    "    plt.figure(figsize=(8, 8), facecolor=None)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_cumulative_variance_ratio <a class=\"anchor\" id=\"seccion-47\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to visually represent the percentage of variance explained by each PCA component. This is a crucial step in selecting a cutoff in the number of PC's to preseve a given proportion of the total variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_cumulative_variance_ratio(pca, n_features):\n",
    "\n",
    "    '''\n",
    "    Function to visually represent the percentage of variance explained by each PCA component\n",
    "\n",
    "    Parameters =\n",
    "\n",
    "    pca: Name of the variable assigned to the PCA\n",
    "    n_features: Number of PCA components\n",
    "\n",
    "    Returns: \n",
    "    Matplotlib lineplot of the variance explained by each PCA component\n",
    "\n",
    "\n",
    "    '''\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_varianceratio)[:n_features]\n",
    "\n",
    "    # Create a bar plot of the cumulative variance ratio\n",
    "    plt.plot(range(1, n_features + 1), cumulative_variance_ratio)\n",
    "    plt.xlabel('Number of Principal Components')\n",
    "    plt.ylabel('Cumulative Variance Ratio')\n",
    "\n",
    "    # Show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_roc_cruve <a class=\"anchor\" id=\"seccion-48\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to plot the ROC curve of a binary classifier. This step is vital to understand both the overall performance of a model but also the trade off betweeen precision and recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred, pos_label=1, figsize=(8, 8)):\n",
    "    '''\n",
    "    Function to plot the ROC curve of a binary classifier\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "    y_true: true labels\n",
    "    y_pred: model predictions\n",
    "    pos_label: positive label (default: 1)\n",
    "    figsize: figure size (default: (8, 8))\n",
    "\n",
    "    Returns: \n",
    "    Lineplot of the ROC curve\n",
    "    \n",
    "    '''\n",
    "    # Compute the false positive rate, true positive rate, and thresholds\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred, pos_label=pos_label)\n",
    "\n",
    "    # Compute the area under the curve (AUC)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Create the ROC curve plot\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic (ROC) curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot_multiclass_prediction_image <a class=\"anchor\" id=\"seccion-49\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common after building a multiclass image classifier that we want to assess how well our predictions have done by visualising the image alongside the real label, the top prediction and probabilities of the predictions of all classes. This allows us to assess where and posisble why our model may be failing. This function creates two plots side by side relating to a prediction of an image. This function can take as an argument the output of the function `create_multiclass_prediction_df()` directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiclass_prediction_image(df, row_index: int, X_test: Union[pd.DataFrame, np.ndarray], prediction_col: str = 'Top Prediction', label_col: str = 'Label'):\n",
    "    '''\n",
    "    This will produce two plots side by side relating to a prediction of an image\n",
    "    The first is the image titled with the label and the predicted label\n",
    "    The second is a bar plot showing the actual probabilities of predictions for all classes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        The dataset containing the results of the prediction along with the label and top predicted label\n",
    "    row_index : int\n",
    "        the index value for the image we want to plot\n",
    "    X_test : pd.DataFrame or np.ndarray\n",
    "        The X_test data\n",
    "    prediction_col : str\n",
    "        Defaults to 'Top Prediction' but should be the name of column with the prediction for image\n",
    "    label_col : str\n",
    "        Defaults to 'Label' but should be the name of column with the real label\n",
    "    \n",
    "    Return\n",
    "    ----------\n",
    "    displays plots\n",
    "\n",
    "    '''\n",
    "\n",
    "    to_plot = df.loc[[row_index]]\n",
    "\n",
    "    fig, ax = plt.subplots(1, 2, figsize = (10, 5), constrained_layout = True)\n",
    "\n",
    "    ax[0].imshow(X_test[row_index])\n",
    "    ax[0].set_title(f'Label: {df.loc[row_index, prediction_col]}\\nPrediction: {df.loc[row_index, label_col]}')\n",
    "\n",
    "    df.loc[row_index, df.dtypes != object].plot.bar(ax = ax[1])\n",
    "    plt.xticks(rotation = 45, ha = 'right')\n",
    "    plt.title('Probabilities of Each Class')\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging and Deployment Description <a class=\"anchor\" id=\"seccion-50\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the library PyPI we followed the following steps:\n",
    "\n",
    "1. File packaging:\n",
    "   \n",
    "   To package the library files, we did the following:\n",
    "   \n",
    "- Made sure we had a good file and directory structure for the project. This included having all the modules within the main library folder (ds11mltoolkit) with the library split into 4 modules representing the 4 main parts of a data science mchine learning pipeline.\n",
    "- We then created a new file setup.py at the base of the project where we have a description, version, authors and other information as shown below:\n",
    "\n",
    "\n",
    "```python\n",
    "from distutils.core import setup\n",
    "\n",
    "setup(\n",
    "name = 'ds11mltoolkit',\n",
    "packages = ['ds11mltoolkit'],\n",
    "version = '1.0',\n",
    "license = 'MIT',\n",
    "description = 'Helper functions for all stages of the machine learning model building process',\n",
    "author = 'TheBridgeMachineLearningPythonLibrary',\n",
    "author_email = 'seenstevol@protonmail.com',\n",
    "url = 'https://github.com/TheBridgeMachineLearningPythonLibrary/MachineLearningToolKit',\n",
    "download_url = 'https://github.com/TheBridgeMachineLearningPythonLibrary/MachineLearningToolKit/archive/refs/tags/V_1.tar.gz',\n",
    "keywords = ['machine learning', 'data visualization', 'data processing', 'sklearn', 'pandas'],\n",
    "install_requires=[\n",
    "'pandas',\n",
    "'numpy',\n",
    "'imblearn',\n",
    "'sklearn',\n",
    "'matplotlib'\n",
    "],\n",
    "classifiers=[\n",
    "'Development Status :: 3 - Alpha',\n",
    "'Intended Audience :: Developers',\n",
    "'Intended Audience :: Education',\n",
    "'Intended Audience :: Science/Research',\n",
    "'Topic :: Scientific/Engineering',\n",
    "'License :: OSI Approved :: MIT License',\n",
    "'Programming Language :: Python :: 3.7',\n",
    "],\n",
    ")\n",
    "```\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then ran the following command to generate the dist/ folder which contains the `.tar.gz` of the library:\n",
    "\n",
    "```python\n",
    "python setup.py sdist\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Publish on PyPI <a class=\"anchor\" id=\"seccion-51\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the library ready to deploy we followed the following to setup the `PyPI` account:\n",
    "\n",
    " Registered an account with PyPI.\n",
    " Installed twine on one persons computer in charge of the upload to PyPI using pip followed by the upload command:\n",
    "\n",
    "```python\n",
    "pip install twine\n",
    "\n",
    "twine upload dist/*\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uploads the `.tar.gz` file in the *dist/* folder containing all the files of the library.\n",
    "\n",
    "Once uploaded, the library was available for download by pip install to be tested and used.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last conclusions <a class=\"anchor\" id=\"seccion-52\"></a>\n",
    "<div style=\"position: fixed; top: 10px; right: 10px;\">\n",
    "    <a href=\"#indice\">Back to index</a>\n",
    "</div>\n",
    "\n",
    "[Back to index](#indice)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project has been our first experience working in a numerous group of people for most of us, and we can agree it has been an experience we will not forget easily. On the one hand, we have made a deep review of everything we have learnt across this Bootcamp, from pure Python to Machine Learning models or Pypi. Besides, we had a great organization developing each participants strong points, a good attitude and effort. However, we also had to face some challenges, like working against the time, GitHub commands, or our new biggest enemy *TESTING*!\n",
    "\n",
    "Focusing now on the library, we have created a very complete and robust library with a lot of functions according to the necessities of each participant this last months. We all think it would have been useful to have a library like this during our learning stages, so it will be also an interesting tool for new Data Science fans like us."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "207c308bf4f500f1746affa728cb3678c6c9733dbbb4e5042cb4d1135561ef1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
